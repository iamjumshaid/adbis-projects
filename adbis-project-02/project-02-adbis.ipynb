{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f98b9b5-d1b2-492b-8aa0-13353410d816",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Advanced Databases and Information Systems Project I</span>\n",
    "## Implementation of Join Algorithm for SPARQL Query Processing\n",
    "\n",
    "**Group Members**:\n",
    "* Omar Swelam os132@uni-freiburg.de\n",
    "* Jumshaid Khan jk1308@uni-freiburg.de\n",
    "\n",
    "**Submitted to**: \n",
    "Dr. Fang Wei-Kleiner\n",
    "\n",
    "**Repository:** https://github.com/iamjumshaid/adbis-projects\n",
    "\n",
    "**<span style=\"color:red\">Note:</span>** We have added your email `fwei@informatik.uni-freiburg.de` as collaborator to our private GitHub repository.\n",
    "  \n",
    "**Date:** 31.07.2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e958fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "import numpy\n",
    "import os\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d734f",
   "metadata": {},
   "source": [
    "# Tasks on small dataset \"watdiv100k.txt\"\n",
    "\n",
    "* **<span style=\"color:red\">Note:</span>** All three tasks are done on larger dataset file \"watdiv.10M.tar.bz2\" at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2e4ba4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 01\n",
    "The first task is to pre-process the data. It is required to partition the triples into relations by using\n",
    "vertically partitioned approach, namely for each distinct property, set up a table with ’Subject’ and\n",
    "’Object’ as columns. Assume there are n properties in the triple store, then you need to construct n\n",
    "tables. One optional step before the pre-processing, is to build up a dictionary of all strings occurring\n",
    "in the triple store and transform the string values into integers. Since the comparison of integer is\n",
    "much faster than the comparison on string values, this optional step helps improve the efficiency of\n",
    "the join algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e240f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('100k.txt', sep='\\t', header=None, names=['Subject', 'Property','Object'])\n",
    "df['Object'] = df.Object.str.rstrip(\" .\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b0cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Property\"] = df.Property.str.split(\":\").apply(lambda x: x[1])\n",
    "df[\"Object\"] = df.Object.str.split(\":\").apply(lambda x: x[1] if len(x)>1 else x[0])\n",
    "df[\"Subject\"] = df.Subject.str.split(\":\").apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ee09e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The code snippet groups data in DataFrame df based on the 'Property' column, creating a dictionary\n",
    "# property_dicts with unique 'Property' values as keys and corresponding subsets as DataFrames.\n",
    "# It displays the first two rows of each group and stores the subsets in the dictionary.\n",
    "# This enables efficient access and manipulation of data related to specific properties.\n",
    "\n",
    "property_dicts = {}\n",
    "for prop, df_part in df.groupby('Property'):\n",
    "    print(prop)\n",
    "    print(df_part.head(2))\n",
    "    print(\"========================\")\n",
    "    property_dicts[prop] = df_part[[\"Subject\",\"Object\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feda91ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code snippet identifies properties within property_dicts where all the elements in the 'Object' \n",
    "# value are numeric strings and converts those numeric strings to integer values. The names of such properties\n",
    "# are stored in the numeric_objects list for further use.\n",
    "\n",
    "numeric_objects = []\n",
    "for k_prop in property_dicts.keys():\n",
    "    if property_dicts[k_prop]['Object'].str.isnumeric().all():\n",
    "        numeric_objects.append(k_prop)\n",
    "        property_dicts[k_prop]['Object'] = property_dicts[k_prop]['Object'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951fc9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(numeric_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f52667",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dfs = ['follows', 'friendOf', 'likes', 'hasReview']\n",
    "# finding whether the property tables have unique elements types i.e. users, products, reviews, ...\n",
    "# would be helpful to transform strings into integers and join them\n",
    "\n",
    "for prop in list_of_dfs:\n",
    "    print(f\"The {prop} has the following unique values:\")\n",
    "    print(\"Subject: \" + str(property_dicts[prop]['Subject'].apply(lambda x: x[:4]).unique()))\n",
    "    print(\"Object: \" + str(property_dicts[prop]['Object'].apply(lambda x: x[:4]).unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8386348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "# we convert the string values to integers for the tables we will use in the join operations\n",
    "\n",
    "def extract_integer_part(input_string):\n",
    "    pattern = r'\\d+'  # This regex pattern matches one or more digits in the string.\n",
    "    match = re.search(pattern, input_string)\n",
    "    if match:\n",
    "        return int(match.group())  # Convert the matched substring to an integer.\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5aa63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prop in list_of_dfs:\n",
    "    property_dicts[prop]['Subject'] = property_dicts[prop]['Subject'].apply(extract_integer_part)\n",
    "    property_dicts[prop]['Object'] = property_dicts[prop]['Object'].apply(extract_integer_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2863a28",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa877892",
   "metadata": {},
   "source": [
    "The second task is to design and implement hash join and sort-merge join algorithms for the query\n",
    "evaluation. Obviously, our running query can be expressed in the form of SQL given the data set\n",
    "yield by vertically partitioned approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97e4aba",
   "metadata": {},
   "source": [
    "**Hash join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba62093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Hash join we have 2 main steps, the first one is to build the hash_map from the first table, and the second one is \n",
    "# probing the second table to match each one with the corresponding hash_key elements and avoiding collisions\n",
    "\n",
    "def build(table, join_key, hash_function):\n",
    "# Builds a hash table from the input table by applying the hash function to the join key column.\n",
    "# Partitions the table into sub-tables based on the distinct hash keys, and stores them in a dictionary.\n",
    "# Returns the hash table, which maps hash keys to corresponding sub-tables.\n",
    "\n",
    "    table['hash_key'] = table[join_key].apply(hash_function)\n",
    "    hash_table = dict(tuple(table.groupby('hash_key')))\n",
    "    table.drop('hash_key', axis=1, inplace=True)\n",
    "    return hash_table\n",
    "\n",
    "\n",
    "\n",
    "def probe(hash_table, join_key1, table2, join_key2, hash_function):\n",
    "# For each row in the second table, calculates the hash key using the hash function on its join key value.\n",
    "# If a matching hash key exists in the hash table, it extracts the corresponding sub-table and filters rows where join key values match.\n",
    "# Appends the second table's columns to the matching rows and stores the resulting sub-tables in a list.\n",
    "# Concatenates all the sub-tables into a single DataFrame and returns it.\n",
    "\n",
    "    dfs_to_join = []\n",
    "    for _, row in table2.iterrows():\n",
    "        hash_key = hash_function(row[join_key2])\n",
    "        if hash_key in hash_table:\n",
    "            df_hash = hash_table[hash_key].copy()\n",
    "            df_hash = df_hash[df_hash[join_key1] == row[join_key2]]\n",
    "            for col in row.index:\n",
    "                df_hash[col + '_2'] = row[col]\n",
    "            dfs_to_join.append(df_hash)\n",
    "    joined_tables = pd.concat(dfs_to_join,axis=0)\n",
    "    return joined_tables\n",
    "\n",
    "def hash_join(table1, table2, join_key1, join_key2, hash_function=hash, join_type='inner'):\n",
    "# Performs a Hash Join operation between two input tables based on their respective join keys.\n",
    "# Handles different join types (inner, left, or right) based on the provided argument.\n",
    "# Creates a hash table from the first table using the build function.\n",
    "# Merges the two tables using the hash table and the probe step.\n",
    "# Returns the resulting merged DataFrame.\n",
    "\n",
    "    if join_type == 'right':\n",
    "        temp = table1\n",
    "        table1 = table2\n",
    "        table2 = temp\n",
    "        temp_key = join_key1\n",
    "        join_key1 = join_key2\n",
    "        join_key2 = temp_key\n",
    "    \n",
    "    hash_table = build(table1, join_key1, hash_function)\n",
    "    joint = probe(hash_table, join_key1, table2, join_key2, hash_function).drop('hash_key', axis=1)\n",
    "    \n",
    "    if join_type in ['right', 'left']:\n",
    "        return pd.concat([joint, table1[~table1[join_key1].apply(lambda x: x in joint[join_key1].values)]],axis=0)\n",
    "    \n",
    "    return joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff0ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result from applying hash_join\n",
    "start_time = time.time()\n",
    "hashed_res_1 = hash_join(property_dicts['follows'], property_dicts['friendOf'],'Object','Subject')\n",
    "hashed_res_1 = hashed_res_1.rename({'Subject':'User', 'Object':'follows', 'Object_2':'friendsOf'}, axis=1).drop('Subject_2', axis=1)\n",
    "hashed_res_2 = hash_join(hashed_res_1, property_dicts['likes'],'friendsOf','Subject')\n",
    "hashed_res_2 = hashed_res_2.rename({'Object_2':'likes'}, axis=1).drop('Subject_2', axis=1)\n",
    "hashed_res_3 = hash_join(hashed_res_2, property_dicts['hasReview'],'likes','Subject')\n",
    "hashed_result = hashed_res_3.rename({'Object_2':'hasReview'}, axis=1).drop('Subject_2', axis=1)\n",
    "end_time = time.time()\n",
    "\n",
    "hashed_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe3a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('time taken: %s seconds' % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b1ed76",
   "metadata": {},
   "source": [
    "**Sort-merge join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf88d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Sort-Merge Join algorithm efficiently merges two tables based on their common join key,\n",
    "# taking advantage of the sorted order to optimize the merge process.\n",
    "\n",
    "def sort_merge_join(table1, table2, join_key1, join_key2,join_type='inner'):\n",
    "# in the first part we start with handling the input:\n",
    "# It supports different join types (inner, left, or right) based on the provided argument (join_type).\n",
    "# If join_type is 'right', it swaps table1 and table2 along with their corresponding join keys to handle right join cases.\n",
    "    \n",
    "\n",
    "    if join_type == 'right':\n",
    "        temp = table1\n",
    "        table1 = table2\n",
    "        table2 = temp\n",
    "        temp_key = join_key1\n",
    "        join_key1 = join_key2\n",
    "        join_key2 = temp_key\n",
    "\n",
    "# The function sorts both table1 and table2 based on their join keys (join_key1 and join_key2, respectively).\n",
    "    sorted_table1 = table1.sort_values(join_key1)\n",
    "    sorted_table2 = table2.sort_values(join_key2)\n",
    "    \n",
    "# the merging part\n",
    "# The function initializes two pointers (pointer1 and pointer2) to track the current position while iterating through the sorted tables.\n",
    "# It initializes an empty list called result to store the merged rows.\n",
    "\n",
    "    pointer1 = 0 \n",
    "    pointer2 = 0\n",
    "    result = []\n",
    "    \n",
    "    while True:\n",
    "        condition1 = pointer1 < len(sorted_table1) \n",
    "        condition2 = pointer2 < len(sorted_table2)\n",
    "        \n",
    "# For an inner join, the loop continues as long as both sorted_table1 and sorted_table2 have remaining elements to be processed.\n",
    "# For left and right joins, the loop continues as long as there are elements in either sorted_table1 or sorted_table2, depending on the join type.\n",
    "        \n",
    "        if join_type=='inner' and not (condition1 and condition2):\n",
    "            break\n",
    "        \n",
    "        if join_type in ['right','left']:\n",
    "            if condition1 and not condition2:\n",
    "                table1_remaining = sorted_table1[~sorted_table1[join_key1].isin(table2.sort_values(join_key2))]\n",
    "                indices, rowSeries = zip(*table1_remaining.add_suffix('_1').iterrows())\n",
    "                result.extend(list(rowSeries))\n",
    "                break\n",
    "            elif not condition1:\n",
    "                break\n",
    "\n",
    "        value1 = sorted_table1.iloc[pointer1][join_key1]\n",
    "        value2 = sorted_table2.iloc[pointer2][join_key2]\n",
    "\n",
    "\n",
    "# The function compares the values from the current positions (pointer1 and pointer2) of sorted_table1 and sorted_table2.\n",
    "# If the values are equal, it means there is a match for the join key, and the corresponding rows from both tables are concatenated and added to the result list.\n",
    "        if value1 == value2:\n",
    "            result.append(pd.concat([sorted_table1.iloc[pointer1].add_suffix('_1'), sorted_table2.iloc[pointer2].add_suffix('_2')]))\n",
    "            \n",
    "            skip_condition1 = (pointer2 < len(sorted_table2)-1) and (sorted_table2.iloc[pointer2+1][join_key2] != value2)\n",
    "            skip_condition2 = (pointer1 < len(sorted_table1)-1) and (sorted_table1.iloc[pointer1+1][join_key1] != value1)\n",
    "            \n",
    "# After a match is found (when value1 == value2), the function checks if there are more occurrences of the current join key in either table. \n",
    "\n",
    "# It does this by checking if the next element in sorted_table1 has the same join key value as the \n",
    "# current element (skip_condition1), and similarly for sorted_table2 (skip_condition2).\n",
    "\n",
    "# If skip_condition1 is True, it means that the next row in sorted_table1 has a different join key value than the current one. \n",
    "# In this case, pointer1 is incremented (pointer1 += 1), effectively moving to the next distinct join key value in sorted_table1.\n",
    "# Similarly, if skip_condition2 is True, it means that the next row in sorted_table2 has a different join key value than the\n",
    "# current one. In this case, pointer2 is incremented (pointer2 += 1), effectively moving to the next distinct join key value in sorted_table2.\n",
    "\n",
    "            if skip_condition1:\n",
    "                pointer1 += 1\n",
    "            if skip_condition2:\n",
    "                pointer2 += 1\n",
    "            if (not skip_condition1) and (not skip_condition2):\n",
    "                if (pointer2 >= len(sorted_table2)-1):\n",
    "                    pointer2 += 1\n",
    "                elif (pointer1 >= len(sorted_table1)-1):\n",
    "                    pointer1 +=1\n",
    "                else:\n",
    "                    pointer2_checkpoint = pointer2 \n",
    "                    value2_checkpoint = value2\n",
    "                    while True:\n",
    "                        pointer2 += 1\n",
    "                        if(pointer2 == len(sorted_table2)):\n",
    "                            break\n",
    "                        value2 = sorted_table2.iloc[pointer2][join_key2]\n",
    "                        if value2 != value2_checkpoint:\n",
    "                            break\n",
    "                        result.append(pd.concat([sorted_table1.iloc[pointer1].add_suffix('_1'), sorted_table2.iloc[pointer2].add_suffix('_2')]))\n",
    "\n",
    "                    pointer2 = pointer2_checkpoint\n",
    "                    pointer1 += 1\n",
    "                    \n",
    "        elif value1 < value2:\n",
    "                pointer1 += 1\n",
    "        else:\n",
    "                pointer2 += 1\n",
    "                \n",
    "    return pd.DataFrame(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc8178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result from applying sort_merge_join\n",
    "start_time = time.time()\n",
    "merged_res_1 = sort_merge_join(property_dicts['follows'], property_dicts['friendOf'],'Object','Subject')\n",
    "merged_res_1 = merged_res_1.rename({'Subject_1':'User', 'Object_1':'follows', 'Object_2':'friendsOf'}, axis=1).drop('Subject_2', axis=1)\n",
    "merged_res_2 = sort_merge_join(merged_res_1, property_dicts['likes'],'friendsOf','Subject')\n",
    "merged_res_2 = merged_res_2.rename({'Object_2':'likes'}, axis=1).drop('Subject_2', axis=1)\n",
    "merged_res_3 = sort_merge_join(merged_res_2, property_dicts['hasReview'],'likes','Subject')\n",
    "merged_result = merged_res_3.rename({'Object_2':'hasReview'}, axis=1).drop('Subject_2', axis=1)\n",
    "end_time = time.time()\n",
    "\n",
    "merged_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('time taken: %s seconds' % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3932050d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece272a",
   "metadata": {},
   "source": [
    "The third task is to design and implement an improvement algorithm regarding the running time.\n",
    "There is no restrictions on the approaches. Possible candidates are: use radix join algorithm, use\n",
    "a different hash function or hashing scheme, partition the data before the join operation, or use\n",
    "parallel sorting algorithms. Other options are for instance building indexes on the data before the\n",
    "query evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17819d6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Radix join not improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5528b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def radix_hash_function(join_key, num_buckets, radix_level):\n",
    "    # Extract the specified radix level from the join key.\n",
    "    radix_value = join_key // (num_buckets ** radix_level) % num_buckets\n",
    "    return radix_value\n",
    "\n",
    "def radix_partition(table, join_key, num_buckets, radix_level):\n",
    "    # Create a new column for each radix level.\n",
    "    for r in range(radix_level):\n",
    "        table[f'hash_key_{r}'] = table[join_key].apply(radix_hash_function, args=(num_buckets, r))\n",
    "\n",
    "    # Group the rows based on radix levels.\n",
    "    groups = table.groupby([f'hash_key_{r}' for r in range(radix_level)])\n",
    "\n",
    "# Returns the dictionary of buckets where each bucket has a key in the of the combination of radix levels values, and the values\n",
    "# in the dict are the rows corresponding to these radix values\n",
    "\n",
    "    buckets = {}\n",
    "    for i, (group_key, group) in enumerate(groups):\n",
    "        buckets[group_key] = group.to_dict('records')\n",
    "\n",
    "    return buckets\n",
    "\n",
    "def chained_radix_join(table1, table2, join_key1, join_key2, num_buckets, radix_level):\n",
    "# Calls radix_partition on both tables to obtain buckets of rows organized based on their radix values for the specified radix level.    \n",
    "    table1 = table1.rename(columns={\"Subject\": \"Subject_1\", \"Object\": \"Object_1\"})\n",
    "    table2 = table2.rename(columns={\"Subject\": \"Subject_2\", \"Object\": \"Object_2\"})\n",
    "    buckets1 = radix_partition(table1, f\"{join_key1}_1\", num_buckets, radix_level)\n",
    "    buckets2 = radix_partition(table2, f\"{join_key2}_2\", num_buckets, radix_level)\n",
    "    \n",
    "\n",
    "# Iterates through the radix buckets of table1, and for each radix value, checks if there is a corresponding radix bucket in table2 with the same value.\n",
    "\n",
    "    merged_tables = []\n",
    "    for hash_value in buckets1.keys(): # changed in here\n",
    "        if hash_value not in buckets2.keys():\n",
    "            continue\n",
    "        inner_buckets1 = buckets1[hash_value]\n",
    "        inner_buckets2 = buckets2[hash_value]\n",
    "        \n",
    "# If a match is found, it performs an additional check on each row of both buckets to ensure that the join keys match for all radix levels.\n",
    "# Concatenates the matching rows from both tables and stores them in a list called merged_tables.\n",
    "\n",
    "        for inner_row1 in inner_buckets1:\n",
    "            for inner_row2 in inner_buckets2:\n",
    "                # Check if join keys match for all radix levels.\n",
    "                match = True\n",
    "                for r in range(radix_level):\n",
    "                    if inner_row1[f'hash_key_{r}'] != inner_row2[f'hash_key_{r}']:\n",
    "                        match = False\n",
    "                        break\n",
    "\n",
    "                if match:\n",
    "                    # Concatenate matching rows from both tables.\n",
    "                    merged_row = pd.concat([pd.Series(inner_row1), pd.Series(inner_row2)], axis=0)\n",
    "                    merged_tables.append(merged_row.drop([f'hash_key_{r}' for r in range(radix_level)]))\n",
    "\n",
    "    if merged_tables:\n",
    "        result = pd.DataFrame(merged_tables)\n",
    "    else:\n",
    "        result = pd.DataFrame()\n",
    "\n",
    "    \n",
    "    result = result[result[f\"{join_key1}_1\"] == result[f\"{join_key2}_2\"]]\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d9d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result from applying hash_join\n",
    "start_time = time.time()\n",
    "rad_res_1 = chained_radix_join(property_dicts['follows'], property_dicts['friendOf'],'Object','Subject')\n",
    "rad_res_1 = rad_res_1.rename({'Subject':'User', 'Object':'follows', 'Object_2':'friendsOf'}, axis=1).drop('Subject_2', axis=1)\n",
    "rad_res_2 = chained_radix_join(rad_res_1, property_dicts['likes'],'friendsOf','Subject')\n",
    "rad_res_2 = rad_res_2.rename({'Object_2':'likes'}, axis=1).drop('Subject_2', axis=1)\n",
    "rad_res_3 = chained_radix_join(rad_res_2, property_dicts['hasReview'],'likes','Subject')\n",
    "rad_result = rad_res_3.rename({'Object_2':'hasReview'}, axis=1).drop('Subject_2', axis=1)\n",
    "end_time = time.time()\n",
    "\n",
    "rad_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb668d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('time taken: %s seconds' % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18cac7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Radix join improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85788e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the faster version\n",
    "\n",
    "def radix_hash_function(join_key, num_buckets, radix_level):\n",
    "    # Extract the specified radix level from the join key.\n",
    "    radix_value = join_key // (num_buckets ** radix_level) % num_buckets\n",
    "    return radix_value\n",
    "\n",
    "def radix_partition(table, join_key, num_buckets, radix_level):\n",
    "    # Create a new column for each radix level.\n",
    "    for r in range(radix_level):\n",
    "        table[f'hash_key_{r}'] = table[join_key].apply(radix_hash_function, args=(num_buckets, r))\n",
    "    \n",
    "    # Group the rows based on radix levels.\n",
    "    groups = table.groupby([f'hash_key_{r}' for r in range(radix_level)])\n",
    "\n",
    "    buckets = {}\n",
    "    for group_key, group in groups:\n",
    "        buckets[group_key] = group.set_index([f'hash_key_{r}' for r in range(radix_level)])\n",
    "    return buckets\n",
    "\n",
    "def chained_radix_join(table1, table2, join_key1, join_key2, num_buckets, radix_level):\n",
    "    table1 = table1.rename(columns={\"Subject\": \"Subject_1\", \"Object\": \"Object_1\"})\n",
    "    table2 = table2.rename(columns={\"Subject\": \"Subject_2\", \"Object\": \"Object_2\"})\n",
    "    buckets1 = radix_partition(table1, f\"{join_key1}_1\", num_buckets, radix_level)\n",
    "    buckets2 = radix_partition(table2, f\"{join_key2}_2\", num_buckets, radix_level)\n",
    "    \n",
    "    merged_tables = []\n",
    "    for hash_value in buckets1.keys(): # changed in here\n",
    "        if hash_value not in buckets2.keys():\n",
    "            continue\n",
    "        inner_buckets1 = buckets1[hash_value]\n",
    "        inner_buckets2 = buckets2[hash_value]\n",
    "# instead of going through the buckets row by row, we just join all the tables with same hash values using pandas join based on the index of the tables\n",
    "        merged_tables.append(inner_buckets1.join(inner_buckets2))\n",
    "    if merged_tables:\n",
    "        result = pd.concat(merged_tables)\n",
    "    else:\n",
    "        result = pd.DataFrame()\n",
    "# here we filter out values that are not the same to avoid collision\n",
    "    result = result[result[f\"{join_key1}_1\"] == result[f\"{join_key2}_2\"]]\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d94715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result from applying hash_join\n",
    "start_time = time.time()\n",
    "rad_res_1 = chained_radix_join(property_dicts['follows'], property_dicts['friendOf'],'Object','Subject')\n",
    "rad_res_1 = rad_res_1.rename({'Subject':'User', 'Object':'follows', 'Object_2':'friendsOf'}, axis=1).drop('Subject_2', axis=1)\n",
    "rad_res_2 = chained_radix_join(rad_res_1, property_dicts['likes'],'friendsOf','Subject')\n",
    "rad_res_2 = rad_res_2.rename({'Object_2':'likes'}, axis=1).drop('Subject_2', axis=1)\n",
    "rad_res_3 = chained_radix_join(rad_res_2, property_dicts['hasReview'],'likes','Subject')\n",
    "rad_result = rad_res_3.rename({'Object_2':'hasReview'}, axis=1).drop('Subject_2', axis=1)\n",
    "end_time = time.time()\n",
    "\n",
    "rad_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('time taken: %s seconds' % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267c33da-6375-484d-bd60-b2973bfde7cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Task 1 with 'watdiv.10M.nt' Big Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e77a86-7468-48dd-be84-6869d48c4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we read the data in batch of 100. so the memory is never overflooded regardless of the data size.\n",
    "# also, after reading the data, we make one file for each property and neglect all the other properties\n",
    "# that are not necessary to save memory size and processing time.\n",
    "\n",
    "file_path = 'watdiv.10M.nt'\n",
    "batch_size = 100\n",
    "file_path_follows = 'follows.txt'\n",
    "file_path_friendOf = 'friendOf.txt'\n",
    "file_path_likes = 'likes.txt'\n",
    "file_path_hasReview = 'hasReview.txt'\n",
    "\n",
    "# Function to delete a file if it exists\n",
    "def delete_if_exists(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Deleted '{file_path}'.\")\n",
    "    else:\n",
    "        print(f\"'{file_path}' does not exist.\")\n",
    "\n",
    "# Delete the files if they exist\n",
    "delete_if_exists(file_path_follows)\n",
    "delete_if_exists(file_path_friendOf)\n",
    "delete_if_exists(file_path_likes)\n",
    "delete_if_exists(file_path_hasReview)\n",
    "\n",
    "\n",
    "# Read the file in chunks using pd.read_csv() and process each chunk\n",
    "# No need to extract the property in seperated files\n",
    "# Because we have each seperate file for each property type\n",
    "for df_chunk in pd.read_csv(file_path, chunksize=batch_size, sep='\\t', header=None, names=['Subject', 'Property', 'Object']):\n",
    "    df_chunk['Property'] = df_chunk['Property'].str.extract(r'[#/]([^#/>]+)>\\s*$')\n",
    "    filtered_df = df_chunk.loc[df_chunk['Property'].str.contains('follows|friendOf|likes|hasReview')]\n",
    "    if len(filtered_df):\n",
    "        if (filtered_df['Property'] == 'follows').sum():\n",
    "            filtered_df[filtered_df['Property'] == 'follows'][['Subject', 'Object']].to_csv(file_path_follows, mode='a', index=False, header=False)\n",
    "        \n",
    "        if (filtered_df['Property'] == 'friendOf').sum():\n",
    "            filtered_df[filtered_df['Property'] == 'friendOf'][['Subject', 'Object']].to_csv(file_path_friendOf, mode='a', index=False, header=False)\n",
    "        \n",
    "        if (filtered_df['Property'] == 'likes').sum():\n",
    "            filtered_df[filtered_df['Property'] == 'likes'][['Subject', 'Object']].to_csv(file_path_likes, mode='a', index=False, header=False)\n",
    "        \n",
    "        if (filtered_df['Property'] == 'hasReview').sum():\n",
    "            filtered_df[filtered_df['Property'] == 'hasReview'][['Subject', 'Object']].to_csv(file_path_hasReview, mode='a', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3523b57-1f35-49dc-973c-b9af8aa020fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_dfs = ['follows', 'friendOf', 'likes', 'hasReview']\n",
    "property_dicts_big = {}\n",
    "\n",
    "for prop in list_of_dfs:\n",
    "    property_dicts_big[prop] = pd.read_csv(f'{prop}.txt', header=None, names=['Subject','Object'])\n",
    "    property_dicts_big[prop]['Object'] = property_dicts_big[prop].Object.str.rstrip(\" .\")\n",
    "    property_dicts_big[prop]['Object'] = property_dicts_big[prop]['Object'].str.extract(r'[#/]([^#/>]+)>\\s*$')\n",
    "    property_dicts_big[prop]['Subject'] = property_dicts_big[prop]['Subject'].str.extract(r'[#/]([^#/>]+)>\\s*$')\n",
    "\n",
    "property_dicts_big['follows']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7eb92b-381b-4430-93ce-3c432b3b5e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding whether the property tables have unique elements types i.e. users, products, reviews, ...\n",
    "# would be helpful to transform strings into integers and join them\n",
    "\n",
    "for prop in list_of_dfs:\n",
    "    print(f\"The {prop} has the following unique values:\")\n",
    "    print(\"Subject: \" + str(property_dicts_big[prop]['Subject'].apply(lambda x: x[:4]).unique()))\n",
    "    print(\"Object: \" + str(property_dicts_big[prop]['Object'].apply(lambda x: x[:4]).unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def extract_integer_part(input_string):\n",
    "    pattern = r'\\d+'  # This regex pattern matches one or more digits in the string.\n",
    "    match = re.search(pattern, input_string)\n",
    "    if match:\n",
    "        return int(match.group())  # Convert the matched substring to an integer.\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87276c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prop in list_of_dfs:\n",
    "    property_dicts_big[prop]['Subject'] = property_dicts_big[prop]['Subject'].apply(extract_integer_part)\n",
    "    property_dicts_big[prop]['Object'] = property_dicts_big[prop]['Object'].apply(extract_integer_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e77a45",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Task 2 with 'watdiv.10M.nt' Big Data File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6975a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hash join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa655286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result from applying hash_join\n",
    "start_time = time.time()\n",
    "hashed_res_1_big = hash_join(property_dicts_big['follows'], property_dicts_big['friendOf'],'Object','Subject')\n",
    "hashed_res_1_big = hashed_res_1_big.rename({'Subject':'User', 'Object':'follows', 'Object_2':'friendsOf'}, axis=1).drop('Subject_2', axis=1)\n",
    "hashed_res_2_big = hash_join(hashed_res_1_big, property_dicts_big['likes'],'friendsOf','Subject')\n",
    "hashed_res_2_big = hashed_res_2_big.rename({'Object_2':'likes'}, axis=1).drop('Subject_2', axis=1)\n",
    "hashed_res_3_big = hash_join(hashed_res_2_big, property_dicts_big['hasReview'],'likes','Subject')\n",
    "hashed_result_big = hashed_res_3_big.rename({'Object_2':'hasReview'}, axis=1).drop('Subject_2', axis=1)\n",
    "end_time = time.time()\n",
    "\n",
    "hashed_result_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689dfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('time taken: %s seconds' % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d38059e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Task 3 Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f156f15e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Radix improved join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bd4f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result from applying hash_join\n",
    "start_time = time.time()\n",
    "rad_res_1_big = chained_radix_join(property_dicts_big['follows'], property_dicts_big['friendOf'],'Object','Subject')\n",
    "rad_res_1_big = rad_res_1_big.rename({'Subject':'User', 'Object':'follows', 'Object_2':'friendsOf'}, axis=1).drop('Subject_2', axis=1)\n",
    "rad_res_2_big = chained_radix_join(rad_res_1_big, property_dicts_big['likes'],'friendsOf','Subject')\n",
    "rad_res_2_big = rad_res_2_big.rename({'Object_2':'likes'}, axis=1).drop('Subject_2', axis=1)\n",
    "rad_res_3_big = chained_radix_join(rad_res_2_big, property_dicts_big['hasReview'],'likes','Subject')\n",
    "rad_result_big = rad_res_3_big.rename({'Object_2':'hasReview'}, axis=1).drop('Subject_2', axis=1)\n",
    "end_time = time.time()\n",
    "\n",
    "rad_result_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('time taken: %s seconds' % (end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
